{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning facebook:wav2vec2-large-960h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we finetune the facebook:wav2vec2-large-960h model from huggingface using the `cv-valid-train` common_voice dataset. This notebook follows the finetuning framework from this [hugginface blog](https://huggingface.co/blog/fine-tune-wav2vec2-english) with minor adaptations. First, we import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import gc\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Audio as PlayAudio\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset, Audio, DatasetDict, load_from_disk, Dataset\n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "from mutagen import File\n",
    "\n",
    "from jiwer import wer\n",
    "\n",
    "HOME_DIR = os.path.expanduser('~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert all mp3 files to wav files, which the wav2vec2 model assumes. Additionally converting transcript texts to upper case to match the original model. This may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File locations. All files assumed placed in asr_proejct folder\n",
    "audio_or_dir = os.path.join(HOME_DIR,'asr_project/common_voice/cv-valid-train/')\n",
    "audio_dir = os.path.join(HOME_DIR,'asr_project/common_voice/cv-valid-train/cv-valid-train/')\n",
    "audioloc_transcript_or_dir = os.path.join(HOME_DIR,'asr_project/common_voice/cv-valid-train.csv')\n",
    "audioloc_transcript_dir = os.path.join(HOME_DIR,'asr_project/asr-train/selected_transcript.csv')\n",
    "temp_dir = os.path.join(HOME_DIR,'asr_project/asr-train/temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to convert mp3 to wav\n",
    "# def convert_mp3_to_wav(mp3_file):\n",
    "#     # Generate the output wav file path\n",
    "#     wav_file = mp3_file.replace('.mp3', '.wav')\n",
    "    \n",
    "#     # Convert mp3 to wav if wav file does not exist\n",
    "#     if not os.path.exists(wav_file):\n",
    "#         waveform, sample_rate = torchaudio.load(mp3_file)\n",
    "#         torchaudio.save(wav_file, waveform, sample_rate)\n",
    "    \n",
    "#     return wav_file\n",
    "\n",
    "\n",
    "# df = pd.read_csv(audioloc_transcript_or_dir)\n",
    "\n",
    "# # Convert mp3 to wav. Change mp3 file extension in df accordingly\n",
    "# df['filename'] = df['filename'].apply(\n",
    "#     lambda filename: convert_mp3_to_wav(\n",
    "#         os.path.join(audio_or_dir, filename)))\n",
    "\n",
    "# # Put texts to uppercase to match pre-finetuned model\n",
    "# df['text'] = df['text'].str.upper()\n",
    "# df['filename'] = df['filename'].map(lambda x: os.path.basename(x))\n",
    "\n",
    "# df_transcript = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking audio file characteristics ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_audio_info(file_path):\n",
    "#     # Extract filename and extension\n",
    "#     file_name, file_ext = os.path.splitext(os.path.basename(file_path))\n",
    "#     file_size = os.path.getsize(file_path)  # Size in bytes\n",
    "\n",
    "#     # Try to get audio length with mutagen\n",
    "#     try:\n",
    "#         audio = File(file_path)\n",
    "#         audio_length = audio.info.length if audio and audio.info else None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not process file {file_name}: {e}\")\n",
    "#         audio_length = None\n",
    "\n",
    "#     return {\n",
    "#         'filename': file_name,\n",
    "#         'extension': file_ext,\n",
    "#         'size_bytes': file_size,\n",
    "#         'length_seconds': audio_length\n",
    "#     }\n",
    "\n",
    "# def process_directory(directory):\n",
    "#     # List all audio files in directory\n",
    "#     audio_files = [\n",
    "#         os.path.join(directory, f) for f in os.listdir(directory) \n",
    "#         if os.path.isfile(os.path.join(directory, f))\n",
    "#     ]\n",
    "\n",
    "#     # Use tqdm with multiprocessing\n",
    "#     with Pool(cpu_count()) as pool:\n",
    "#         # Wrap audio files list with tqdm for progress bar\n",
    "#         audio_info = list(tqdm(pool.imap(get_audio_info, audio_files), total=len(audio_files), desc=\"Processing files\"))\n",
    "\n",
    "#     # Create DataFrame from the list of dictionaries\n",
    "#     df = pd.DataFrame(audio_info)\n",
    "#     return df\n",
    "\n",
    "# # Get audio file information\n",
    "# audio_df = process_directory(audio_dir)\n",
    "# audio_df_mp3 = audio_df.loc[audio_df['extension']=='.mp3'].copy()\n",
    "# audio_df_wav = audio_df.loc[audio_df['extension']=='.wav'].copy().drop(columns=['length_seconds'])\n",
    "# audio_df_wav = audio_df_wav.merge(audio_df_mp3[['filename','length_seconds']], on='filename', how='left')\n",
    "# audio_df_wav['filename'] = audio_df_wav['filename'].map(lambda x: x+'.wav')\n",
    "# audio_df_wav.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some of them have very high durations, up to 6 minutes long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_df_wav.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the transcript, we find that the longest line read is only 33 words long, which should not take that long to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transcript['len'] = df_transcript['text'].str.len()\n",
    "# df_transcript = df_transcript[['filename','len','text']]\n",
    "\n",
    "# filename_longest = df_transcript.loc[df_transcript['len']==df_transcript['len'].max(), 'filename'].item()\n",
    "# text_longest = df_transcript.loc[df_transcript['len']==df_transcript['len'].max(), 'text'].item()\n",
    "\n",
    "# print(f'Filename with longest transcript: {filename_longest}')\n",
    "# print(f'Longest transcript text: {text_longest}')\n",
    "\n",
    "# longest_clip_duration = audio_df_wav.loc[audio_df_wav['filename']==filename_longest,'length_seconds'].item()\n",
    "# print(f'Longest transcript duration: {longest_clip_duration}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clip with the longest transcript is 11s long. Considering differences in reading speeds, we assume the longest legitimate script reading to be 15s long. __We discard all samples with durations above 15s__. This will help prevent memory issues during model finetuning. We drop a total of 397 samples, keeping ~195k samples, saving a copy as csv file for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transcript = df_transcript.merge(audio_df_wav[['filename', 'length_seconds']], on='filename', how='left')\n",
    "# (df_transcript['length_seconds'] > 15).sum().item(),  (df_transcript['length_seconds'] < 15).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transcript = df_transcript.loc[df_transcript['length_seconds']<15].drop(columns=['len','length_seconds'])\n",
    "# df_transcript.to_csv(audioloc_transcript_dir, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `DatasetDict` for easy access to train-val splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load csv file with wav filenames, complete path and create dataset\n",
    "# df = pd.read_csv(audioloc_transcript_dir)\n",
    "# df['filename'] = df['filename'].map(lambda x: os.path.join(audio_dir,x))\n",
    "# df.to_csv(temp_dir,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset('csv', data_files=temp_dir, split='train')\n",
    "# dataset = dataset.cast_column(\"filename\",\n",
    "#                               Audio(sampling_rate=16000))         # Cast audio files with 16kHz sampling rate\n",
    "\n",
    "# # train-val 70-30 split\n",
    "# dataset = dataset.train_test_split(test_size=0.3, seed=42)        # Split to train-val\n",
    "\n",
    "# # Final, combined dataset\n",
    "# dataset = DatasetDict({\n",
    "#     'train': dataset['train'],\n",
    "#     'val': dataset['test']})\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of the tokenizer and processor from `facebook/wav2vec2-large-960h` in the model finetuning below. First, the transcripts are converted to the format expected by the model. The transcript have already been converted into uppercase earlier for this purpose. We insert start, end, and delimited tokens below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Following the style of facebook/wav2ec2-large-960h model\n",
    "# start_token = \"<s>\"\n",
    "# end_token = \"</s>\"\n",
    "# word_delimiter_token = \"|\"\n",
    "\n",
    "# # Define the preprocessing function\n",
    "# def preprocess_transcript(example):\n",
    "#     transcript = example['text']  # Assuming the column with text is named 'text'\n",
    "    \n",
    "#     # Step 1: Replace multiple spaces with a single space\n",
    "#     transcript = re.sub(r'\\s+', ' ', transcript)  # Remove extra spaces\n",
    "    \n",
    "#     # Step 2: Add start and end tokens, and replace spaces with '|'\n",
    "#     processed_transcript = start_token + transcript.replace(\" \", f\"{word_delimiter_token}\") + end_token\n",
    "    \n",
    "#     return {\"processed_text\": processed_transcript}  # Return the processed text in a dictionary\n",
    "\n",
    "# # Apply the preprocessing to both train and validation splits\n",
    "# dataset = dataset.map(preprocess_transcript, remove_columns=[\"text\"],num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to column names expected by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.rename_column(\"filename\", \"input_values\")\n",
    "# dataset = dataset.rename_column(\"processed_text\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tokenize the transcripts and use the `input_values` and `labels` column names in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load processor\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "# def prepare_dataset(batch):\n",
    "#     # Process 'input_values' column for 1D waveform values\n",
    "#     batch[\"input_values\"] = processor(batch[\"input_values\"][\"array\"],\n",
    "#                                       sampling_rate=16000).input_values[0]\n",
    "    \n",
    "#     # Process the 'labels' column to create 'labels' (text data)\n",
    "#     batch[\"labels\"] = processor(text=batch[\"labels\"]).input_ids\n",
    "    \n",
    "#     return batch\n",
    "\n",
    "# # Map the dataset transformation to both 'train' and 'val' splits\n",
    "# dataset = dataset.map(prepare_dataset, num_proc=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the dataset to a directory\n",
    "# dataset.save_to_disk(\"temp_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick check, play a random audio file below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_int = random.randint(0, len(dataset[\"train\"]))\n",
    "# print(dataset[\"train\"][\"labels\"][rand_int])\n",
    "\n",
    "# audio_data = dataset[\"train\"][rand_int][\"input_values\"]\n",
    "# PlayAudio(data=audio_data, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and check the data formats, e.g. 1-D waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_int = random.randint(0, len(dataset[\"train\"]))\n",
    "\n",
    "# print(\"Target (encoded) text:\", dataset[\"train\"][rand_int][\"labels\"])\n",
    "# print(\"Input array shape:\", np.asarray(dataset[\"train\"][rand_int][\"input_values\"]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As elaborated [here](https://huggingface.co/blog/fine-tune-wav2vec2-english), a data collator with dynamic padding is more efficient for ASR applications, considering the lengths of the input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_from_disk(\"temp_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the WER metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def remove_start_end_tags(texts):\n",
    "    # Remove the <s> and </s> tags from both ends of each string\n",
    "    return [re.sub(r\"^<s>|</s>$\", \"\", text) for text in texts]\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    # Replace padding token id with -100\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and references\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    # Remove the <s> and </s> tags from the decoded strings\n",
    "    pred_str = remove_start_end_tags(pred_str)\n",
    "    label_str = remove_start_end_tags(label_str)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-960h\", \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# # Freeze feature extractor layers\n",
    "# model.freeze_feature_encoder()\n",
    "\n",
    "# Freeze all layers except the head\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze all parameters\n",
    "\n",
    "# Assuming the head is the `classifier` in Wav2Vec2ForCTC\n",
    "for param in model.lm_head.parameters():  # For the head (classifier) layer\n",
    "    param.requires_grad = True  # Unfreeze the head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get a baseline WER for a quick comparison with the finetuned model's performance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_to_result(batch):\n",
    "#     with torch.no_grad():\n",
    "#         input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "#         logits = model(input_values).logits\n",
    "\n",
    "#     pred_ids = torch.argmax(logits, dim=-1)\n",
    "#     batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "#     batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "#     return batch\n",
    "\n",
    "# model.to('cuda')\n",
    "# results = dataset[\"val\"].map(map_to_result, remove_columns=dataset[\"val\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_start_end_tags(batch):\n",
    "#     # Remove the <s> and </s> tags from both ends of each string in 'pred_str' and 'text'\n",
    "#     batch[\"pred_str\"] = re.sub(r\"^<s>|</s>$\", \"\", batch[\"pred_str\"])\n",
    "#     batch[\"text\"] = re.sub(r\"^<s>|</s>$\", \"\", batch[\"text\"])\n",
    "#     return batch\n",
    "\n",
    "# # Apply the function to the entire dataset\n",
    "# results = results.map(remove_start_end_tags)\n",
    "\n",
    "\n",
    "# print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model shows a WER of about 10.5% before finetuning. We complete the setup for the huggingface trainer and begin training below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2137 [00:00<?, ?it/s]/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      " 12%|█▏        | 250/2137 [05:12<38:47,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.9285, 'grad_norm': 4.3315043449401855, 'learning_rate': 1.2450000000000001e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 500/2137 [10:20<29:53,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.3718, 'grad_norm': 3.421187162399292, 'learning_rate': 2.495e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 23%|██▎       | 500/2137 [19:23<29:53,  1.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.665811061859131, 'eval_wer': 0.1104161030247281, 'eval_runtime': 542.9331, 'eval_samples_per_second': 107.958, 'eval_steps_per_second': 6.749, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 750/2137 [24:13<28:57,  1.25s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.5016, 'grad_norm': 3.1834044456481934, 'learning_rate': 3.745e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1000/2137 [29:04<22:49,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.5337, 'grad_norm': 2.6247568130493164, 'learning_rate': 4.995e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 47%|████▋     | 1000/2137 [35:58<22:49,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6904981136322021, 'eval_wer': 0.11616434897101528, 'eval_runtime': 413.9829, 'eval_samples_per_second': 141.586, 'eval_steps_per_second': 8.851, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1250/2137 [41:07<16:26,  1.11s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5242, 'grad_norm': 2.016247510910034, 'learning_rate': 3.9050131926121375e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1500/2137 [46:22<13:34,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.7639, 'grad_norm': 1.742484450340271, 'learning_rate': 2.805628847845207e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 70%|███████   | 1500/2137 [53:04<13:34,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2211542129516602, 'eval_wer': 0.1202514293959562, 'eval_runtime': 401.3354, 'eval_samples_per_second': 146.047, 'eval_steps_per_second': 9.13, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1750/2137 [58:12<08:27,  1.31s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2705, 'grad_norm': 2.2663869857788086, 'learning_rate': 1.706244503078276e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 2000/2137 [1:03:24<02:56,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.9872, 'grad_norm': 2.0656380653381348, 'learning_rate': 6.068601583113457e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 94%|█████████▎| 2000/2137 [1:10:00<02:56,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0519750118255615, 'eval_wer': 0.1210215897408149, 'eval_runtime': 396.6229, 'eval_samples_per_second': 147.783, 'eval_steps_per_second': 9.238, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [1:12:38<00:00,  2.04s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4358.677, 'train_samples_per_second': 31.377, 'train_steps_per_second': 0.49, 'train_loss': 7.080849418586585, 'epoch': 1.0}\n",
      "Time: 4358.68\n",
      "Samples/second: 31.38\n",
      "GPU memory occupied: 14071 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LEARNED T RECOGNIE OMEN AND  FOLLOW THEM THEOLD  KING HAD SAID']\n"
     ]
    }
   ],
   "source": [
    "# Load audio file\n",
    "audio_file = \"/home/tfc/asr_project/common_voice/cv-valid-train/cv-valid-train/sample-000000.wav\"  # Replace with your audio file path\n",
    "waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "\n",
    "# If the sample rate is not 16kHz, resample it\n",
    "if sample_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "# Convert to the right format for the model\n",
    "input_values = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "\n",
    "# Get logits from the model\n",
    "model.to('cpu')\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# Get predicted ids\n",
    "predicted_ids = logits.argmax(dim=-1)\n",
    "\n",
    "# Decode the predicted ids to text\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "print(transcription)  # Print the transcription result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path where the fine-tuned model is saved\n",
    "model_dir = os.path.expanduser('~/asr_project/asr-train/model_outputs/checkpoint-2137')\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_dir\n",
    ")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "    return batch\n",
    "\n",
    "model.to('cuda')\n",
    "results = dataset[\"val\"].map(map_to_result, remove_columns=dataset[\"val\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 58614/58614 [00:00<00:00, 73247.36 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 0.121\n"
     ]
    }
   ],
   "source": [
    "def remove_start_end_tags(batch):\n",
    "    # Remove the <s> and </s> tags from both ends of each string in 'pred_str' and 'text'\n",
    "    batch[\"pred_str\"] = re.sub(r\"^<s>|</s>$\", \"\", batch[\"pred_str\"])\n",
    "    batch[\"text\"] = re.sub(r\"^<s>|</s>$\", \"\", batch[\"text\"])\n",
    "    return batch\n",
    "\n",
    "# Apply the function to the entire dataset\n",
    "results = results.map(remove_start_end_tags)\n",
    "\n",
    "\n",
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))\n",
    "# WER of 0.12 for eval set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 136764/136764 [55:19<00:00, 41.20 examples/s] \n"
     ]
    }
   ],
   "source": [
    "def map_to_result(batch):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "    return batch\n",
    "\n",
    "model.to('cuda')\n",
    "results = dataset[\"train\"].map(map_to_result, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 136764/136764 [00:02<00:00, 66451.57 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train WER: 0.120\n"
     ]
    }
   ],
   "source": [
    "def remove_start_end_tags(batch):\n",
    "    # Remove the <s> and </s> tags from both ends of each string in 'pred_str' and 'text'\n",
    "    batch[\"pred_str\"] = re.sub(r\"^<s>|</s>$\", \"\", batch[\"pred_str\"])\n",
    "    batch[\"text\"] = re.sub(r\"^<s>|</s>$\", \"\", batch[\"text\"])\n",
    "    return batch\n",
    "\n",
    "# Apply the function to the entire dataset\n",
    "results = results.map(remove_start_end_tags)\n",
    "\n",
    "\n",
    "print(\"train WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))\n",
    "# WER of 0.12 for train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
