{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning facebook:wav2vec2-large-960h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we finetune the facebook:wav2vec2-large-960h model from huggingface using the `cv-valid-train` common_voice dataset. This notebook follows the finetuning framework from this [hugginface blog](https://huggingface.co/blog/fine-tune-wav2vec2-english) with minor adaptations. First, we import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import gc\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Audio as PlayAudio\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset, Audio, DatasetDict, load_from_disk, Dataset\n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "from mutagen import File\n",
    "\n",
    "from jiwer import wer\n",
    "\n",
    "HOME_DIR = os.path.expanduser('~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert all mp3 files to wav files, which the wav2vec2 model assumes. Additionally converting transcript texts to upper case to match the original model. This may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File locations. All files assumed placed in asr_proejct folder\n",
    "audio_or_dir = os.path.join(HOME_DIR,'asr_project/common_voice/cv-valid-train/')\n",
    "audio_dir = os.path.join(HOME_DIR,'asr_project/common_voice/cv-valid-train/cv-valid-train/')\n",
    "audioloc_transcript_or_dir = os.path.join(HOME_DIR,'asr_project/common_voice/cv-valid-train.csv')\n",
    "audioloc_transcript_dir = os.path.join(HOME_DIR,'asr_project/asr-train/selected_transcript.csv')\n",
    "temp_dir = os.path.join(HOME_DIR,'asr_project/asr-train/temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to convert mp3 to wav\n",
    "# def convert_mp3_to_wav(mp3_file):\n",
    "#     # Generate the output wav file path\n",
    "#     wav_file = mp3_file.replace('.mp3', '.wav')\n",
    "    \n",
    "#     # Convert mp3 to wav if wav file does not exist\n",
    "#     if not os.path.exists(wav_file):\n",
    "#         waveform, sample_rate = torchaudio.load(mp3_file)\n",
    "#         torchaudio.save(wav_file, waveform, sample_rate)\n",
    "    \n",
    "#     return wav_file\n",
    "\n",
    "\n",
    "# df = pd.read_csv(audioloc_transcript_or_dir)\n",
    "\n",
    "# # Convert mp3 to wav. Change mp3 file extension in df accordingly\n",
    "# df['filename'] = df['filename'].apply(\n",
    "#     lambda filename: convert_mp3_to_wav(\n",
    "#         os.path.join(audio_or_dir, filename)))\n",
    "\n",
    "# # Put texts to uppercase to match pre-finetuned model\n",
    "# df['text'] = df['text'].str.upper()\n",
    "# df['filename'] = df['filename'].map(lambda x: os.path.basename(x))\n",
    "\n",
    "# df_transcript = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking audio file characteristics ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_audio_info(file_path):\n",
    "#     # Extract filename and extension\n",
    "#     file_name, file_ext = os.path.splitext(os.path.basename(file_path))\n",
    "#     file_size = os.path.getsize(file_path)  # Size in bytes\n",
    "\n",
    "#     # Try to get audio length with mutagen\n",
    "#     try:\n",
    "#         audio = File(file_path)\n",
    "#         audio_length = audio.info.length if audio and audio.info else None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not process file {file_name}: {e}\")\n",
    "#         audio_length = None\n",
    "\n",
    "#     return {\n",
    "#         'filename': file_name,\n",
    "#         'extension': file_ext,\n",
    "#         'size_bytes': file_size,\n",
    "#         'length_seconds': audio_length\n",
    "#     }\n",
    "\n",
    "# def process_directory(directory):\n",
    "#     # List all audio files in directory\n",
    "#     audio_files = [\n",
    "#         os.path.join(directory, f) for f in os.listdir(directory) \n",
    "#         if os.path.isfile(os.path.join(directory, f))\n",
    "#     ]\n",
    "\n",
    "#     # Use tqdm with multiprocessing\n",
    "#     with Pool(cpu_count()) as pool:\n",
    "#         # Wrap audio files list with tqdm for progress bar\n",
    "#         audio_info = list(tqdm(pool.imap(get_audio_info, audio_files), total=len(audio_files), desc=\"Processing files\"))\n",
    "\n",
    "#     # Create DataFrame from the list of dictionaries\n",
    "#     df = pd.DataFrame(audio_info)\n",
    "#     return df\n",
    "\n",
    "# # Get audio file information\n",
    "# audio_df = process_directory(audio_dir)\n",
    "# audio_df_mp3 = audio_df.loc[audio_df['extension']=='.mp3'].copy()\n",
    "# audio_df_wav = audio_df.loc[audio_df['extension']=='.wav'].copy().drop(columns=['length_seconds'])\n",
    "# audio_df_wav = audio_df_wav.merge(audio_df_mp3[['filename','length_seconds']], on='filename', how='left')\n",
    "# audio_df_wav['filename'] = audio_df_wav['filename'].map(lambda x: x+'.wav')\n",
    "# audio_df_wav.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some of them have very high durations, up to 6 minutes long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_df_wav.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the transcript, we find that the longest line read is only 33 words long, which should not take that long to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transcript['len'] = df_transcript['text'].str.len()\n",
    "# df_transcript = df_transcript[['filename','len','text']]\n",
    "\n",
    "# filename_longest = df_transcript.loc[df_transcript['len']==df_transcript['len'].max(), 'filename'].item()\n",
    "# text_longest = df_transcript.loc[df_transcript['len']==df_transcript['len'].max(), 'text'].item()\n",
    "\n",
    "# print(f'Filename with longest transcript: {filename_longest}')\n",
    "# print(f'Longest transcript text: {text_longest}')\n",
    "\n",
    "# longest_clip_duration = audio_df_wav.loc[audio_df_wav['filename']==filename_longest,'length_seconds'].item()\n",
    "# print(f'Longest transcript duration: {longest_clip_duration}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clip with the longest transcript is 11s long. Considering differences in reading speeds, we assume the longest legitimate script reading to be 15s long. __We discard all samples with durations above 15s__. This will help prevent memory issues during model finetuning. We drop a total of 397 samples, keeping ~195k samples, saving a copy as csv file for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transcript = df_transcript.merge(audio_df_wav[['filename', 'length_seconds']], on='filename', how='left')\n",
    "# (df_transcript['length_seconds'] > 15).sum().item(),  (df_transcript['length_seconds'] < 15).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transcript = df_transcript.loc[df_transcript['length_seconds']<15].drop(columns=['len','length_seconds'])\n",
    "# df_transcript.to_csv(audioloc_transcript_dir, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `DatasetDict` for easy access to train-val splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load csv file with wav filenames, complete path and create dataset\n",
    "# df = pd.read_csv(audioloc_transcript_dir)\n",
    "# df['filename'] = df['filename'].map(lambda x: os.path.join(audio_dir,x))\n",
    "# df.to_csv(temp_dir,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset('csv', data_files=temp_dir, split='train')\n",
    "# dataset = dataset.cast_column(\"filename\",\n",
    "#                               Audio(sampling_rate=16000))         # Cast audio files with 16kHz sampling rate\n",
    "\n",
    "# # train-val 70-30 split\n",
    "# dataset = dataset.train_test_split(test_size=0.3, seed=42)        # Split to train-val\n",
    "\n",
    "# # Final, combined dataset\n",
    "# dataset = DatasetDict({\n",
    "#     'train': dataset['train'],\n",
    "#     'val': dataset['test']})\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of the tokenizer and processor from `facebook/wav2vec2-large-960h` in the model finetuning below. First, the transcripts are converted to the format expected by the model. The transcript have already been converted into uppercase earlier for this purpose. We insert start, end, and delimited tokens below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Following the style of facebook/wav2ec2-large-960h model\n",
    "# start_token = \"<s>\"\n",
    "# end_token = \"</s>\"\n",
    "# word_delimiter_token = \"|\"\n",
    "\n",
    "# # Define the preprocessing function\n",
    "# def preprocess_transcript(example):\n",
    "#     transcript = example['text']  # Assuming the column with text is named 'text'\n",
    "    \n",
    "#     # Step 1: Replace multiple spaces with a single space\n",
    "#     transcript = re.sub(r'\\s+', ' ', transcript)  # Remove extra spaces\n",
    "    \n",
    "#     # Step 2: Add start and end tokens, and replace spaces with '|'\n",
    "#     processed_transcript = start_token + transcript.replace(\" \", f\"{word_delimiter_token}\") + end_token\n",
    "    \n",
    "#     return {\"processed_text\": processed_transcript}  # Return the processed text in a dictionary\n",
    "\n",
    "# # Apply the preprocessing to both train and validation splits\n",
    "# dataset = dataset.map(preprocess_transcript, remove_columns=[\"text\"],num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to column names expected by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.rename_column(\"filename\", \"input_values\")\n",
    "# dataset = dataset.rename_column(\"processed_text\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tokenize the transcripts and use the `input_values` and `labels` column names in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load processor\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "# def prepare_dataset(batch):\n",
    "#     # Process 'input_values' column for 1D waveform values\n",
    "#     batch[\"input_values\"] = processor(batch[\"input_values\"][\"array\"],\n",
    "#                                       sampling_rate=16000).input_values[0]\n",
    "    \n",
    "#     # Process the 'labels' column to create 'labels' (text data)\n",
    "#     batch[\"labels\"] = processor(text=batch[\"labels\"]).input_ids\n",
    "    \n",
    "#     return batch\n",
    "\n",
    "# # Map the dataset transformation to both 'train' and 'val' splits\n",
    "# dataset = dataset.map(prepare_dataset, num_proc=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the dataset to a directory\n",
    "# dataset.save_to_disk(\"temp_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick check, play a random audio file below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_int = random.randint(0, len(dataset[\"train\"]))\n",
    "# print(dataset[\"train\"][\"labels\"][rand_int])\n",
    "\n",
    "# audio_data = dataset[\"train\"][rand_int][\"input_values\"]\n",
    "# PlayAudio(data=audio_data, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and check the data formats, e.g. 1-D waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_int = random.randint(0, len(dataset[\"train\"]))\n",
    "\n",
    "# print(\"Target (encoded) text:\", dataset[\"train\"][rand_int][\"labels\"])\n",
    "# print(\"Input array shape:\", np.asarray(dataset[\"train\"][rand_int][\"input_values\"]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As elaborated [here](https://huggingface.co/blog/fine-tune-wav2vec2-english), a data collator with dynamic padding is more efficient for ASR applications, considering the lengths of the input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_from_disk(\"temp_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the WER metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def remove_start_end_tags(texts):\n",
    "    # Remove the <s> and </s> tags from both ends of each string\n",
    "    return [re.sub(r\"^<s>|</s>$\", \"\", text) for text in texts]\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    # Replace padding token id with -100\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and references\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    # Remove the <s> and </s> tags from the decoded strings\n",
    "    pred_str = remove_start_end_tags(pred_str)\n",
    "    label_str = remove_start_end_tags(label_str)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-960h\", \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# Freeze all layers initially\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the output layer (lm_head)\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze the last few encoder layers (e.g., last 3)\n",
    "for layer in model.wav2vec2.encoder.layers[-3:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get a baseline WER for a quick comparison with the finetuned model's performance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_to_result(batch):\n",
    "#     with torch.no_grad():\n",
    "#         input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "#         logits = model(input_values).logits\n",
    "\n",
    "#     pred_ids = torch.argmax(logits, dim=-1)\n",
    "#     batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "#     batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "#     return batch\n",
    "\n",
    "# model.to('cuda')\n",
    "# results = dataset[\"val\"].map(map_to_result, remove_columns=dataset[\"val\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_start_end_tags(batch):\n",
    "#     # Remove the <s> and </s> tags from both ends of each string in 'pred_str' and 'text'\n",
    "#     batch[\"pred_str\"] = re.sub(r\"^<s>|</s>$\", \"\", batch[\"pred_str\"])\n",
    "#     batch[\"text\"] = re.sub(r\"^<s>|</s>$\", \"\", batch[\"text\"])\n",
    "#     return batch\n",
    "\n",
    "# # Apply the function to the entire dataset\n",
    "# results = results.map(remove_start_end_tags)\n",
    "\n",
    "\n",
    "# print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model shows a WER of about 10.5% before finetuning. We complete the setup for the huggingface trainer and begin training below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.expanduser('~/asr_project/asr-train/model_outputs'),\n",
    "    logging_dir=os.path.expanduser('~/asr_project/asr-train/logs'),\n",
    "    per_device_train_batch_size=16,              # batch size for training\n",
    "    per_device_eval_batch_size=16,               # batch size for evaluation\n",
    "    num_train_epochs=1,                           # total number of training epochs\n",
    "    logging_steps=250,                            # log every 100 steps\n",
    "    eval_strategy=\"steps\",                  # evaluate during training\n",
    "    save_steps=500,                               # save checkpoint every 500 steps\n",
    "    eval_steps=500,                               # evaluate every 500 steps\n",
    "    warmup_steps=1000,\n",
    "    load_best_model_at_end=True,                  # load the best model at the end of training\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    "    # optim=\"adamw_bnb_8bit\",                # adamw_apex_fused, adamw_bnb_8bit. did not help.\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "# Define huggingface trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"val\"],\n",
    "    processing_class= processor.feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wav2Vec2ForCTC'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2137 [00:00<?, ?it/s]/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      " 12%|█▏        | 250/2137 [05:22<39:45,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.7395, 'grad_norm': 4.505060195922852, 'learning_rate': 1.2450000000000001e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 500/2137 [10:39<30:23,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0132, 'grad_norm': 4.511003494262695, 'learning_rate': 2.495e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 23%|██▎       | 500/2137 [19:42<30:23,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3488627076148987, 'eval_wer': 0.16045307793589814, 'eval_runtime': 543.1204, 'eval_samples_per_second': 107.921, 'eval_steps_per_second': 6.746, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 750/2137 [24:40<29:41,  1.28s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7697, 'grad_norm': 10.393481254577637, 'learning_rate': 3.745e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1000/2137 [29:37<22:06,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1198, 'grad_norm': 3.137352466583252, 'learning_rate': 4.9850000000000006e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 47%|████▋     | 1000/2137 [36:05<22:06,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29072362184524536, 'eval_wer': 0.14237505185505836, 'eval_runtime': 388.4081, 'eval_samples_per_second': 150.908, 'eval_steps_per_second': 9.433, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1250/2137 [41:00<15:56,  1.08s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5175, 'grad_norm': 3.5548977851867676, 'learning_rate': 3.913808267370273e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1500/2137 [46:20<13:46,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3647, 'grad_norm': 2.808368444442749, 'learning_rate': 2.8144239226033426e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 70%|███████   | 1500/2137 [53:13<13:46,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2704225778579712, 'eval_wer': 0.13052504373861443, 'eval_runtime': 413.0359, 'eval_samples_per_second': 141.91, 'eval_steps_per_second': 8.871, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1750/2137 [58:28<08:33,  1.33s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3485, 'grad_norm': 3.753955602645874, 'learning_rate': 1.7150395778364116e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 2000/2137 [1:03:30<02:49,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3177, 'grad_norm': 2.6831319332122803, 'learning_rate': 6.156552330694811e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 94%|█████████▎| 2000/2137 [1:10:01<02:49,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26375553011894226, 'eval_wer': 0.1328842234366827, 'eval_runtime': 390.103, 'eval_samples_per_second': 150.253, 'eval_steps_per_second': 9.392, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [1:12:54<00:00,  2.05s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4374.0358, 'train_samples_per_second': 31.267, 'train_steps_per_second': 0.489, 'train_loss': 2.9741442711151365, 'epoch': 1.0}\n",
      "Time: 4374.04\n",
      "Samples/second: 31.27\n",
      "GPU memory occupied: 14481 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Start training\n",
    "# result = trainer.train()\n",
    "# print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/4274 [00:00<?, ?it/s]/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3026: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      " 53%|█████▎    | 2250/4274 [02:27<41:40,  1.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3308, 'grad_norm': 3.341240167617798, 'learning_rate': 3.0956017104459376e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 2500/4274 [07:44<33:31,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2687, 'grad_norm': 3.068417549133301, 'learning_rate': 2.713805742211362e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 58%|█████▊    | 2500/4274 [17:14<33:31,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26491907238960266, 'eval_wer': 0.12579766607146078, 'eval_runtime': 570.4208, 'eval_samples_per_second': 102.756, 'eval_steps_per_second': 6.423, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 2750/4274 [22:34<32:28,  1.28s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.255, 'grad_norm': 2.675313711166382, 'learning_rate': 2.332009773976787e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3000/4274 [27:54<25:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2402, 'grad_norm': 2.535522937774658, 'learning_rate': 1.9502138057422113e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 70%|███████   | 3000/4274 [34:48<25:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24750325083732605, 'eval_wer': 0.12581750626769836, 'eval_runtime': 413.4692, 'eval_samples_per_second': 141.761, 'eval_steps_per_second': 8.862, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 3250/4274 [40:00<19:39,  1.15s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1998, 'grad_norm': 2.597294330596924, 'learning_rate': 1.568417837507636e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 3500/4274 [44:53<15:02,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1498, 'grad_norm': 2.1874780654907227, 'learning_rate': 1.1866218692730606e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 82%|████████▏ | 3500/4274 [51:22<15:02,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2186083346605301, 'eval_wer': 0.11320455242320943, 'eval_runtime': 388.6141, 'eval_samples_per_second': 150.828, 'eval_steps_per_second': 9.428, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 3750/4274 [56:40<11:27,  1.31s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.159, 'grad_norm': 2.123910903930664, 'learning_rate': 8.048259010384851e-06, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 4000/4274 [1:01:57<06:40,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.124, 'grad_norm': 2.5504770278930664, 'learning_rate': 4.230299328039096e-06, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 94%|█████████▎| 4000/4274 [1:08:26<06:40,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21943162381649017, 'eval_wer': 0.11471601464567213, 'eval_runtime': 389.4681, 'eval_samples_per_second': 150.498, 'eval_steps_per_second': 9.408, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 4250/4274 [1:13:25<00:30,  1.27s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1627, 'grad_norm': 2.203531503677368, 'learning_rate': 4.276114844227245e-07, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4274/4274 [1:13:56<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4436.0208, 'train_samples_per_second': 61.661, 'train_steps_per_second': 0.963, 'train_loss': 1.1009562050119204, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4274, training_loss=1.1009562050119204, metrics={'train_runtime': 4436.0208, 'train_samples_per_second': 61.661, 'train_steps_per_second': 0.963, 'total_flos': 6.849131764028483e+19, 'train_loss': 1.1009562050119204, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load model from the last checkpoint\n",
    "# checkpoint_dir = os.path.expanduser('~/asr_project/asr-train/model_outputs/checkpoint-2137')  # Use the latest checkpoint path\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(checkpoint_dir)\n",
    "\n",
    "# # Freeze all layers initially\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the output layer (lm_head)\n",
    "# for param in model.lm_head.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Unfreeze the last few encoder layers (e.g., last 3)\n",
    "# for layer in model.wav2vec2.encoder.layers[-3:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# # Update number of epochs in training arguments\n",
    "# training_args.num_train_epochs = 2\n",
    "\n",
    "# # Reinitialize Trainer with updated model and training args\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     data_collator=data_collator,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     eval_dataset=dataset[\"val\"],\n",
    "#     processing_class=processor.feature_extractor\n",
    "# )\n",
    "\n",
    "# # Resume training from the checkpoint\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/8548 [00:00<?, ?it/s]/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3026: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      " 53%|█████▎    | 4500/8548 [04:49<1:29:26,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1809, 'grad_norm': 2.4402103424072266, 'learning_rate': 2.6841547429782727e-05, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 53%|█████▎    | 4500/8548 [14:19<1:29:26,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22738496959209442, 'eval_wer': 0.11219089876088956, 'eval_runtime': 570.4487, 'eval_samples_per_second': 102.751, 'eval_steps_per_second': 6.423, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 4750/8548 [19:37<1:19:25,  1.25s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1731, 'grad_norm': 2.899099349975586, 'learning_rate': 2.5185479597244304e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5000/8548 [24:52<1:20:04,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1296, 'grad_norm': 2.8731002807617188, 'learning_rate': 2.3529411764705884e-05, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 58%|█████▊    | 5000/8548 [31:26<1:20:04,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2120986431837082, 'eval_wer': 0.11618238551304944, 'eval_runtime': 394.4865, 'eval_samples_per_second': 148.583, 'eval_steps_per_second': 9.288, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 5250/8548 [36:26<1:08:31,  1.25s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0904, 'grad_norm': 3.3416500091552734, 'learning_rate': 2.187334393216746e-05, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 5500/8548 [41:23<58:36,  1.15s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0395, 'grad_norm': 2.4577999114990234, 'learning_rate': 2.0217276099629043e-05, 'epoch': 2.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 64%|██████▍   | 5500/8548 [47:51<58:36,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21265487372875214, 'eval_wer': 0.10806053063506664, 'eval_runtime': 388.3579, 'eval_samples_per_second': 150.928, 'eval_steps_per_second': 9.435, 'epoch': 2.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 5750/8548 [52:49<58:13,  1.25s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0659, 'grad_norm': 3.351557731628418, 'learning_rate': 1.856120826709062e-05, 'epoch': 2.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 6000/8548 [57:47<49:52,  1.17s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0562, 'grad_norm': 2.679572105407715, 'learning_rate': 1.69051404345522e-05, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 70%|███████   | 6000/8548 [1:04:39<49:52,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2171860784292221, 'eval_wer': 0.11430117417888642, 'eval_runtime': 411.7325, 'eval_samples_per_second': 142.359, 'eval_steps_per_second': 8.899, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 6250/8548 [1:09:51<44:07,  1.15s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0902, 'grad_norm': 2.017369270324707, 'learning_rate': 1.5249072602013778e-05, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6411/8548 [1:13:03<45:35,  1.28s/it]/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      " 76%|███████▌  | 6500/8548 [1:14:48<38:15,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0355, 'grad_norm': 2.2311806678771973, 'learning_rate': 1.3599629040805511e-05, 'epoch': 3.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 76%|███████▌  | 6500/8548 [1:21:16<38:15,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20913703739643097, 'eval_wer': 0.11146222246270945, 'eval_runtime': 388.3927, 'eval_samples_per_second': 150.914, 'eval_steps_per_second': 9.434, 'epoch': 3.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 6750/8548 [1:26:15<35:01,  1.17s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0413, 'grad_norm': 2.4515163898468018, 'learning_rate': 1.1943561208267091e-05, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 7000/8548 [1:31:29<34:13,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0404, 'grad_norm': 2.5019114017486572, 'learning_rate': 1.028749337572867e-05, 'epoch': 3.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 82%|████████▏ | 7000/8548 [1:38:24<34:13,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2149658054113388, 'eval_wer': 0.10827155817686633, 'eval_runtime': 414.9385, 'eval_samples_per_second': 141.259, 'eval_steps_per_second': 8.83, 'epoch': 3.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 7250/8548 [1:43:48<26:40,  1.23s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0333, 'grad_norm': 2.3569676876068115, 'learning_rate': 8.63142554319025e-06, 'epoch': 3.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7500/8548 [1:49:02<20:26,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0213, 'grad_norm': 2.102874517440796, 'learning_rate': 6.975357710651828e-06, 'epoch': 3.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 88%|████████▊ | 7500/8548 [1:55:57<20:26,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21116843819618225, 'eval_wer': 0.10860523420449832, 'eval_runtime': 414.6784, 'eval_samples_per_second': 141.348, 'eval_steps_per_second': 8.836, 'epoch': 3.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 7750/8548 [2:01:16<16:49,  1.26s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0196, 'grad_norm': 2.353893995285034, 'learning_rate': 5.3192898781134075e-06, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 8000/8548 [2:06:32<11:30,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0148, 'grad_norm': 2.1585512161254883, 'learning_rate': 3.663222045574987e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 94%|█████████▎| 8000/8548 [2:13:27<11:30,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21051767468452454, 'eval_wer': 0.10790541637357286, 'eval_runtime': 414.972, 'eval_samples_per_second': 141.248, 'eval_steps_per_second': 8.83, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 8250/8548 [2:18:37<06:08,  1.24s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0615, 'grad_norm': 2.580381393432617, 'learning_rate': 2.0071542130365663e-06, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 8500/8548 [2:23:31<00:54,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0021, 'grad_norm': 2.0965805053710938, 'learning_rate': 3.510863804981452e-07, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 99%|█████████▉| 8500/8548 [2:30:00<00:54,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21086861193180084, 'eval_wer': 0.10771422902801076, 'eval_runtime': 388.5138, 'eval_samples_per_second': 150.867, 'eval_steps_per_second': 9.431, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8548/8548 [2:31:01<00:00,  1.06s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 9061.1013, 'train_samples_per_second': 60.374, 'train_steps_per_second': 0.943, 'train_loss': 1.0316393195773934, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8548, training_loss=1.0316393195773934, metrics={'train_runtime': 9061.1013, 'train_samples_per_second': 60.374, 'train_steps_per_second': 0.943, 'total_flos': 1.3699128739309388e+20, 'train_loss': 1.0316393195773934, 'epoch': 4.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load model from the last checkpoint\n",
    "# checkpoint_dir = os.path.expanduser('~/asr_project/asr-train/model_outputs/checkpoint-4274')  # Use the latest checkpoint path\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(checkpoint_dir)\n",
    "\n",
    "# # Freeze all layers initially\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the output layer (lm_head)\n",
    "# for param in model.lm_head.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Unfreeze the last few encoder layers (e.g., last 3)\n",
    "# for layer in model.wav2vec2.encoder.layers[-3:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# # Update number of epochs in training arguments\n",
    "# training_args.num_train_epochs = 4\n",
    "\n",
    "# # Reinitialize Trainer with updated model and training args\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     data_collator=data_collator,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     eval_dataset=dataset[\"val\"],\n",
    "#     processing_class=processor.feature_extractor\n",
    "# )\n",
    "\n",
    "# # Resume training from the checkpoint\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/12822 [00:00<?, ?it/s]/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3026: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      " 68%|██████▊   | 8750/12822 [04:17<1:24:43,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0228, 'grad_norm': 2.457582473754883, 'learning_rate': 1.7243275249534764e-05, 'epoch': 4.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 9000/12822 [09:30<1:13:15,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0438, 'grad_norm': 2.052823066711426, 'learning_rate': 1.61859245474539e-05, 'epoch': 4.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                      \n",
      " 70%|███████   | 9000/12822 [18:54<1:13:15,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2114071100950241, 'eval_wer': 0.10719658027163033, 'eval_runtime': 563.445, 'eval_samples_per_second': 104.028, 'eval_steps_per_second': 6.503, 'epoch': 4.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 9250/12822 [23:52<1:16:13,  1.28s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0033, 'grad_norm': 2.8403213024139404, 'learning_rate': 1.5128573845373034e-05, 'epoch': 4.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 9500/12822 [28:49<1:01:08,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0472, 'grad_norm': 2.3100128173828125, 'learning_rate': 1.4071223143292167e-05, 'epoch': 4.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                      \n",
      " 74%|███████▍  | 9500/12822 [35:17<1:01:08,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21170203387737274, 'eval_wer': 0.10487347365763036, 'eval_runtime': 388.3628, 'eval_samples_per_second': 150.926, 'eval_steps_per_second': 9.434, 'epoch': 4.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 9750/12822 [40:24<1:07:28,  1.32s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9935, 'grad_norm': 2.910015106201172, 'learning_rate': 1.3013872441211303e-05, 'epoch': 4.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 10000/12822 [45:37<1:03:56,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0026, 'grad_norm': 2.4306607246398926, 'learning_rate': 1.1956521739130435e-05, 'epoch': 4.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 78%|███████▊  | 10000/12822 [52:17<1:03:56,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21248231828212738, 'eval_wer': 0.10545064300272351, 'eval_runtime': 399.4046, 'eval_samples_per_second': 146.753, 'eval_steps_per_second': 9.174, 'epoch': 4.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 10250/12822 [57:30<49:57,  1.17s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0183, 'grad_norm': 2.914501667022705, 'learning_rate': 1.0899171037049568e-05, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 10500/12822 [1:02:46<48:12,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9956, 'grad_norm': 2.7230827808380127, 'learning_rate': 9.841820334968702e-06, 'epoch': 4.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 82%|████████▏ | 10500/12822 [1:09:23<48:12,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21068257093429565, 'eval_wer': 0.10471655574193316, 'eval_runtime': 396.312, 'eval_samples_per_second': 147.899, 'eval_steps_per_second': 9.245, 'epoch': 4.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 10685/12822 [1:13:21<46:53,  1.32s/it]    /home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      " 84%|████████▍ | 10750/12822 [1:14:44<46:27,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9703, 'grad_norm': 2.301846742630005, 'learning_rate': 8.784469632887837e-06, 'epoch': 5.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 11000/12822 [1:19:57<39:30,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9916, 'grad_norm': 2.7691338062286377, 'learning_rate': 7.731348333615295e-06, 'epoch': 5.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 86%|████████▌ | 11000/12822 [1:26:26<39:30,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2057541012763977, 'eval_wer': 0.1043413956676226, 'eval_runtime': 388.8379, 'eval_samples_per_second': 150.741, 'eval_steps_per_second': 9.423, 'epoch': 5.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 11250/12822 [1:31:45<32:42,  1.25s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9525, 'grad_norm': 2.431232213973999, 'learning_rate': 6.673997631534427e-06, 'epoch': 5.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 11500/12822 [1:36:59<25:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9763, 'grad_norm': 2.3123910427093506, 'learning_rate': 5.616646929453562e-06, 'epoch': 5.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 90%|████████▉ | 11500/12822 [1:43:28<25:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20701751112937927, 'eval_wer': 0.10459390725610086, 'eval_runtime': 388.5531, 'eval_samples_per_second': 150.852, 'eval_steps_per_second': 9.43, 'epoch': 5.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 11750/12822 [1:48:27<23:42,  1.33s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9729, 'grad_norm': 2.3558928966522217, 'learning_rate': 4.559296227372695e-06, 'epoch': 5.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 12000/12822 [1:53:34<18:16,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.949, 'grad_norm': 2.784388303756714, 'learning_rate': 3.501945525291829e-06, 'epoch': 5.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 94%|█████████▎| 12000/12822 [2:00:27<18:16,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.208365797996521, 'eval_wer': 0.10535504932994247, 'eval_runtime': 413.0784, 'eval_samples_per_second': 141.896, 'eval_steps_per_second': 8.87, 'epoch': 5.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 12250/12822 [2:05:36<12:10,  1.28s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.992, 'grad_norm': 2.1657822132110596, 'learning_rate': 2.4445948232109627e-06, 'epoch': 5.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 12500/12822 [2:10:48<07:11,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9598, 'grad_norm': 2.343845844268799, 'learning_rate': 1.3872441211300965e-06, 'epoch': 5.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 97%|█████████▋| 12500/12822 [2:17:41<07:11,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20779238641262054, 'eval_wer': 0.10544342838590985, 'eval_runtime': 413.323, 'eval_samples_per_second': 141.812, 'eval_steps_per_second': 8.865, 'epoch': 5.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 12750/12822 [2:22:59<01:30,  1.25s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9321, 'grad_norm': 2.287487268447876, 'learning_rate': 3.2989341904923026e-07, 'epoch': 5.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12822/12822 [2:24:32<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8672.0544, 'train_samples_per_second': 94.624, 'train_steps_per_second': 1.479, 'train_loss': 0.6631917339129189, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12822, training_loss=0.6631917339129189, metrics={'train_runtime': 8672.0544, 'train_samples_per_second': 94.624, 'train_steps_per_second': 1.479, 'total_flos': 2.0542244145253997e+20, 'train_loss': 0.6631917339129189, 'epoch': 6.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load model from the last checkpoint\n",
    "# checkpoint_dir = os.path.expanduser('~/asr_project/asr-train/model_outputs/checkpoint-8548')  # Use the latest checkpoint path\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(checkpoint_dir)\n",
    "\n",
    "# # Freeze all layers initially\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the output layer (lm_head)\n",
    "# for param in model.lm_head.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Unfreeze the last few encoder layers (e.g., last 3)\n",
    "# for layer in model.wav2vec2.encoder.layers[-3:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# # Update number of epochs in training arguments\n",
    "# training_args.num_train_epochs = 6\n",
    "\n",
    "# # Reinitialize Trainer with updated model and training args\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     data_collator=data_collator,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     eval_dataset=dataset[\"val\"],\n",
    "#     processing_class=processor.feature_extractor\n",
    "# )\n",
    "\n",
    "# # Resume training from the checkpoint\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/14959 [00:00<?, ?it/s]/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3026: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 13000/14959 [03:49<41:10,  1.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9961, 'grad_norm': 2.9969520568847656, 'learning_rate': 7.038469804427252e-06, 'epoch': 6.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 87%|████████▋ | 13000/14959 [13:23<41:10,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20599280297756195, 'eval_wer': 0.10529011777861948, 'eval_runtime': 573.9524, 'eval_samples_per_second': 102.123, 'eval_steps_per_second': 6.384, 'epoch': 6.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 13250/14959 [18:29<32:23,  1.14s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9883, 'grad_norm': 2.4438109397888184, 'learning_rate': 6.142990185543377e-06, 'epoch': 6.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 13500/14959 [23:22<29:13,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0059, 'grad_norm': 2.587575912475586, 'learning_rate': 5.247510566659503e-06, 'epoch': 6.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 90%|█████████ | 13500/14959 [29:51<29:13,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2049417942762375, 'eval_wer': 0.10627130566527786, 'eval_runtime': 388.4998, 'eval_samples_per_second': 150.873, 'eval_steps_per_second': 9.431, 'epoch': 6.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 13750/14959 [34:52<23:00,  1.14s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9555, 'grad_norm': 2.2986693382263184, 'learning_rate': 4.352030947775629e-06, 'epoch': 6.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 14000/14959 [39:53<18:34,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9755, 'grad_norm': 2.2702341079711914, 'learning_rate': 3.4565513288917543e-06, 'epoch': 6.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 94%|█████████▎| 14000/14959 [46:37<18:34,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2076079547405243, 'eval_wer': 0.10464440957379652, 'eval_runtime': 404.2197, 'eval_samples_per_second': 145.005, 'eval_steps_per_second': 9.064, 'epoch': 6.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 14250/14959 [51:56<14:36,  1.24s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9613, 'grad_norm': 2.6901979446411133, 'learning_rate': 2.5610717100078804e-06, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 14500/14959 [56:58<08:41,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9814, 'grad_norm': 1.82221257686615, 'learning_rate': 1.665592091124006e-06, 'epoch': 6.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacity of 15.69 GiB of which 1.12 GiB is free. Process 1963 has 249.95 MiB memory in use. Including non-PyTorch memory, this process has 14.00 GiB memory in use. Of the allocated memory 6.75 GiB is allocated by PyTorch, and 6.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     processing_class\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mfeature_extractor\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Resume training from the checkpoint\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:2541\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2541\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:2997\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2995\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2997\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   3000\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:2951\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2950\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2951\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2952\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2954\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3964\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3961\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3963\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3964\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3974\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:4185\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4183\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[1;32m   4184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4185\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4187\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:322\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mappend(tensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:136\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    139\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    140\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:100\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     97\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Now let's fill the result tensor\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtensor1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m result[: tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], : tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m tensor1\n\u001b[1;32m    102\u001b[0m result[tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] :, : tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m tensor2\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacity of 15.69 GiB of which 1.12 GiB is free. Process 1963 has 249.95 MiB memory in use. Including non-PyTorch memory, this process has 14.00 GiB memory in use. Of the allocated memory 6.75 GiB is allocated by PyTorch, and 6.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# # Load model from the last checkpoint\n",
    "# checkpoint_dir = os.path.expanduser('~/asr_project/asr-train/model_outputs/checkpoint-12822')  # Use the latest checkpoint path\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(checkpoint_dir)\n",
    "\n",
    "# # Freeze all layers initially\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the output layer (lm_head)\n",
    "# for param in model.lm_head.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Unfreeze the last few encoder layers (e.g., last 3)\n",
    "# for layer in model.wav2vec2.encoder.layers[-3:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# # Update number of epochs in training arguments\n",
    "# training_args.num_train_epochs = 7\n",
    "\n",
    "# # Reinitialize Trainer with updated model and training args\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     data_collator=data_collator,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     eval_dataset=dataset[\"val\"],\n",
    "#     processing_class=processor.feature_extractor\n",
    "# )\n",
    "\n",
    "# # Resume training from the checkpoint\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/17096 [00:00<?, ?it/s]/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3026: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      " 83%|████████▎ | 14250/17096 [05:18<58:52,  1.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9648, 'grad_norm': 2.717271566390991, 'learning_rate': 8.85934393638171e-06, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 14500/17096 [10:36<52:19,  1.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9849, 'grad_norm': 1.9600456953048706, 'learning_rate': 8.082753479125248e-06, 'epoch': 6.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 85%|████████▍ | 14500/17096 [20:03<52:19,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20853330194950104, 'eval_wer': 0.10566708150713346, 'eval_runtime': 566.7377, 'eval_samples_per_second': 103.423, 'eval_steps_per_second': 6.465, 'epoch': 6.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 14750/17096 [25:25<51:33,  1.32s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9405, 'grad_norm': 2.4875502586364746, 'learning_rate': 7.306163021868788e-06, 'epoch': 6.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 14959/17096 [29:40<41:29,  1.17s/it]/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      " 88%|████████▊ | 15000/17096 [30:31<45:43,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9243, 'grad_norm': 2.2438528537750244, 'learning_rate': 6.529572564612327e-06, 'epoch': 7.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 88%|████████▊ | 15000/17096 [36:59<45:43,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21043290197849274, 'eval_wer': 0.10466424977003409, 'eval_runtime': 388.157, 'eval_samples_per_second': 151.006, 'eval_steps_per_second': 9.439, 'epoch': 7.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 15250/17096 [42:01<35:03,  1.14s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9531, 'grad_norm': 2.1504383087158203, 'learning_rate': 5.752982107355865e-06, 'epoch': 7.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 15500/17096 [46:55<29:23,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9486, 'grad_norm': 2.619666814804077, 'learning_rate': 4.976391650099404e-06, 'epoch': 7.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 91%|█████████ | 15500/17096 [53:23<29:23,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2063273787498474, 'eval_wer': 0.10325559583716609, 'eval_runtime': 388.3554, 'eval_samples_per_second': 150.929, 'eval_steps_per_second': 9.435, 'epoch': 7.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 15750/17096 [58:18<27:41,  1.23s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9469, 'grad_norm': 2.2481210231781006, 'learning_rate': 4.199801192842943e-06, 'epoch': 7.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 16000/17096 [1:03:23<27:52,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9425, 'grad_norm': 3.7141096591949463, 'learning_rate': 3.4232107355864816e-06, 'epoch': 7.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 94%|█████████▎| 16000/17096 [1:10:15<27:52,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20668435096740723, 'eval_wer': 0.10453799397579495, 'eval_runtime': 412.1479, 'eval_samples_per_second': 142.216, 'eval_steps_per_second': 8.89, 'epoch': 7.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 16250/17096 [1:15:33<18:23,  1.30s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9594, 'grad_norm': 2.4642505645751953, 'learning_rate': 2.64662027833002e-06, 'epoch': 7.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 16500/17096 [1:20:41<11:39,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9395, 'grad_norm': 2.6411311626434326, 'learning_rate': 1.8700298210735586e-06, 'epoch': 7.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 97%|█████████▋| 16500/17096 [1:27:21<11:39,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20930160582065582, 'eval_wer': 0.10382374691124217, 'eval_runtime': 399.9234, 'eval_samples_per_second': 146.563, 'eval_steps_per_second': 9.162, 'epoch': 7.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 16750/17096 [1:32:39<07:08,  1.24s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9547, 'grad_norm': 2.010868787765503, 'learning_rate': 1.0934393638170975e-06, 'epoch': 7.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 17000/17096 [1:38:02<02:03,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9545, 'grad_norm': 2.315002679824829, 'learning_rate': 3.1995526838966205e-07, 'epoch': 7.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 99%|█████████▉| 17000/17096 [1:44:55<02:03,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20745795965194702, 'eval_wer': 0.10412315350900925, 'eval_runtime': 413.4886, 'eval_samples_per_second': 141.755, 'eval_steps_per_second': 8.861, 'epoch': 7.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17096/17096 [1:46:54<00:00,  2.67it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6414.256, 'train_samples_per_second': 170.575, 'train_steps_per_second': 2.665, 'train_loss': 0.3533202239604659, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17096, training_loss=0.3533202239604659, metrics={'train_runtime': 6414.256, 'train_samples_per_second': 170.575, 'train_steps_per_second': 2.665, 'total_flos': 2.7391593084284348e+20, 'train_loss': 0.3533202239604659, 'epoch': 8.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load model from the last checkpoint\n",
    "# checkpoint_dir = os.path.expanduser('~/asr_project/asr-train/model_outputs/checkpoint-14000')  # Use the latest checkpoint path\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(checkpoint_dir)\n",
    "\n",
    "# # Freeze all layers initially\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the output layer (lm_head)\n",
    "# for param in model.lm_head.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Unfreeze the last few encoder layers (e.g., last 3)\n",
    "# for layer in model.wav2vec2.encoder.layers[-3:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# # Update number of epochs in training arguments\n",
    "# training_args.num_train_epochs = 8\n",
    "\n",
    "# # Reinitialize Trainer with updated model and training args\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     data_collator=data_collator,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     eval_dataset=dataset[\"val\"],\n",
    "#     processing_class=processor.feature_extractor\n",
    "# )\n",
    "\n",
    "# # Resume training from the checkpoint\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/19233 [00:00<?, ?it/s]/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3026: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      " 90%|████████▉ | 17250/19233 [03:18<43:12,  1.31s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9131, 'grad_norm': 1.9416123628616333, 'learning_rate': 5.457138156090605e-06, 'epoch': 8.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 17500/19233 [08:35<36:21,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9748, 'grad_norm': 2.739497661590576, 'learning_rate': 4.77156803597872e-06, 'epoch': 8.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 91%|█████████ | 17500/19233 [18:01<36:21,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21233467757701874, 'eval_wer': 0.1037588153599192, 'eval_runtime': 565.6404, 'eval_samples_per_second': 103.624, 'eval_steps_per_second': 6.478, 'epoch': 8.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 17750/19233 [23:18<29:45,  1.20s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9327, 'grad_norm': 2.1873226165771484, 'learning_rate': 4.0859979158668346e-06, 'epoch': 8.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 18000/19233 [28:19<23:09,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9456, 'grad_norm': 3.34474515914917, 'learning_rate': 3.40042779575495e-06, 'epoch': 8.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 94%|█████████▎| 18000/19233 [34:47<23:09,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21211670339107513, 'eval_wer': 0.10372093862164745, 'eval_runtime': 388.5157, 'eval_samples_per_second': 150.866, 'eval_steps_per_second': 9.431, 'epoch': 8.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 18250/19233 [39:42<20:05,  1.23s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.928, 'grad_norm': 2.090364456176758, 'learning_rate': 2.714857675643065e-06, 'epoch': 8.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 18500/19233 [44:41<15:43,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9531, 'grad_norm': 3.540621280670166, 'learning_rate': 2.02928755553118e-06, 'epoch': 8.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 96%|█████████▌| 18500/19233 [51:10<15:43,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2098599076271057, 'eval_wer': 0.10465703515322042, 'eval_runtime': 388.7846, 'eval_samples_per_second': 150.762, 'eval_steps_per_second': 9.424, 'epoch': 8.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 18750/19233 [56:20<10:20,  1.29s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9315, 'grad_norm': 3.005525588989258, 'learning_rate': 1.3437174354192948e-06, 'epoch': 8.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 19000/19233 [1:01:41<04:52,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9495, 'grad_norm': 2.4876961708068848, 'learning_rate': 6.581473153074098e-07, 'epoch': 8.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 99%|█████████▉| 19000/19233 [1:08:34<04:52,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.207030788064003, 'eval_wer': 0.10396984290171889, 'eval_runtime': 413.2286, 'eval_samples_per_second': 141.844, 'eval_steps_per_second': 8.867, 'epoch': 8.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19233/19233 [1:13:34<00:00,  4.36it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4414.1513, 'train_samples_per_second': 278.848, 'train_steps_per_second': 4.357, 'train_loss': 0.21548220096410223, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19233, training_loss=0.21548220096410223, metrics={'train_runtime': 4414.1513, 'train_samples_per_second': 278.848, 'train_steps_per_second': 4.357, 'total_flos': 3.08184881463889e+20, 'train_loss': 0.21548220096410223, 'epoch': 9.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load model from the last checkpoint\n",
    "# checkpoint_dir = os.path.expanduser('~/asr_project/asr-train/model_outputs/checkpoint-17096')  # Use the latest checkpoint path\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(checkpoint_dir)\n",
    "\n",
    "# # Freeze all layers initially\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the output layer (lm_head)\n",
    "# for param in model.lm_head.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Unfreeze the last few encoder layers (e.g., last 3)\n",
    "# for layer in model.wav2vec2.encoder.layers[-3:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# # Update number of epochs in training arguments\n",
    "# training_args.num_train_epochs = 9\n",
    "\n",
    "# # Reinitialize Trainer with updated model and training args\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     data_collator=data_collator,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     eval_dataset=dataset[\"val\"],\n",
    "#     processing_class=processor.feature_extractor\n",
    "# )\n",
    "\n",
    "# # Resume training from the checkpoint\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/21370 [00:00<?, ?it/s]/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3026: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 19250/21370 [00:23<00:03, 699.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8868, 'grad_norm': 2.25573992729187, 'learning_rate': 5.220913107511046e-06, 'epoch': 9.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 19500/21370 [05:36<38:42,  1.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9583, 'grad_norm': 2.4730710983276367, 'learning_rate': 4.60726558664703e-06, 'epoch': 9.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 91%|█████████ | 19500/21370 [14:59<38:42,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20745672285556793, 'eval_wer': 0.10408708042494093, 'eval_runtime': 562.5588, 'eval_samples_per_second': 104.192, 'eval_steps_per_second': 6.513, 'epoch': 9.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 19750/21370 [19:58<32:10,  1.19s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9571, 'grad_norm': 2.951198101043701, 'learning_rate': 3.993618065783014e-06, 'epoch': 9.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 20000/21370 [24:57<27:26,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9714, 'grad_norm': 2.5559616088867188, 'learning_rate': 3.3799705449189987e-06, 'epoch': 9.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 94%|█████████▎| 20000/21370 [31:26<27:26,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20588833093643188, 'eval_wer': 0.10400230867738038, 'eval_runtime': 388.7466, 'eval_samples_per_second': 150.777, 'eval_steps_per_second': 9.425, 'epoch': 9.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 20250/21370 [36:27<22:49,  1.22s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9504, 'grad_norm': 2.531064510345459, 'learning_rate': 2.7663230240549828e-06, 'epoch': 9.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 20500/21370 [41:22<17:25,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9335, 'grad_norm': 3.0366787910461426, 'learning_rate': 2.1526755031909673e-06, 'epoch': 9.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacity of 15.69 GiB of which 1.79 GiB is free. Process 1963 has 249.95 MiB memory in use. Including non-PyTorch memory, this process has 13.39 GiB memory in use. Of the allocated memory 6.75 GiB is allocated by PyTorch, and 6.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     processing_class\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mfeature_extractor\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Resume training from the checkpoint\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:2541\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2541\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:2997\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2995\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2997\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   3000\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:2951\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2950\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2951\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2952\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2954\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3964\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3961\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3963\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3964\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3974\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:4185\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4183\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[1;32m   4184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4185\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4187\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:322\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mappend(tensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:136\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    139\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    140\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:100\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     97\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Now let's fill the result tensor\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtensor1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m result[: tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], : tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m tensor1\n\u001b[1;32m    102\u001b[0m result[tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] :, : tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m tensor2\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacity of 15.69 GiB of which 1.79 GiB is free. Process 1963 has 249.95 MiB memory in use. Including non-PyTorch memory, this process has 13.39 GiB memory in use. Of the allocated memory 6.75 GiB is allocated by PyTorch, and 6.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# # Load model from the last checkpoint\n",
    "# checkpoint_dir = os.path.expanduser('~/asr_project/asr-train/model_outputs/checkpoint-19233')  # Use the latest checkpoint path\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(checkpoint_dir)\n",
    "\n",
    "# # Freeze all layers initially\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the output layer (lm_head)\n",
    "# for param in model.lm_head.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Unfreeze the last few encoder layers (e.g., last 3)\n",
    "# for layer in model.wav2vec2.encoder.layers[-3:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# # Update number of epochs in training arguments\n",
    "# training_args.num_train_epochs = 10\n",
    "\n",
    "# # Reinitialize Trainer with updated model and training args\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     data_collator=data_collator,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     eval_dataset=dataset[\"val\"],\n",
    "#     processing_class=processor.feature_extractor\n",
    "# )\n",
    "\n",
    "# # Resume training from the checkpoint\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/21370 [00:00<?, ?it/s]/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/trainer.py:3026: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      " 95%|█████████▍| 20250/21370 [05:22<24:20,  1.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9504, 'grad_norm': 2.5279343128204346, 'learning_rate': 2.7663230240549828e-06, 'epoch': 9.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 20500/21370 [10:37<17:26,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9335, 'grad_norm': 3.031528949737549, 'learning_rate': 2.1526755031909673e-06, 'epoch': 9.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 96%|█████████▌| 20500/21370 [19:44<17:26,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20497949421405792, 'eval_wer': 0.10402575618202478, 'eval_runtime': 546.451, 'eval_samples_per_second': 107.263, 'eval_steps_per_second': 6.705, 'epoch': 9.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 20750/21370 [24:42<11:11,  1.08s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9213, 'grad_norm': 1.9516541957855225, 'learning_rate': 1.5390279823269516e-06, 'epoch': 9.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 21000/21370 [29:39<07:03,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9053, 'grad_norm': 3.339163303375244, 'learning_rate': 9.253804614629357e-07, 'epoch': 9.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 98%|█████████▊| 21000/21370 [36:08<07:03,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20628774166107178, 'eval_wer': 0.10380931767761485, 'eval_runtime': 388.616, 'eval_samples_per_second': 150.828, 'eval_steps_per_second': 9.428, 'epoch': 9.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 21250/21370 [41:07<02:25,  1.21s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.941, 'grad_norm': 2.509145975112915, 'learning_rate': 3.1173294059892003e-07, 'epoch': 9.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21370/21370 [43:30<00:00,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2610.4209, 'train_samples_per_second': 523.916, 'train_steps_per_second': 8.186, 'train_loss': 0.12362917523881684, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21370, training_loss=0.12362917523881684, metrics={'train_runtime': 2610.4209, 'train_samples_per_second': 523.916, 'train_steps_per_second': 8.186, 'total_flos': 3.424054837545321e+20, 'train_loss': 0.12362917523881684, 'epoch': 10.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load model from the last checkpoint\n",
    "# checkpoint_dir = os.path.expanduser('~/asr_project/asr-train/model_outputs/checkpoint-20000')  # Use the latest checkpoint path\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(checkpoint_dir)\n",
    "\n",
    "# # Freeze all layers initially\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the output layer (lm_head)\n",
    "# for param in model.lm_head.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Unfreeze the last few encoder layers (e.g., last 3)\n",
    "# for layer in model.wav2vec2.encoder.layers[-3:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# # Update number of epochs in training arguments\n",
    "# training_args.num_train_epochs = 10\n",
    "\n",
    "# # Reinitialize Trainer with updated model and training args\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     data_collator=data_collator,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     eval_dataset=dataset[\"val\"],\n",
    "#     processing_class=processor.feature_extractor\n",
    "# )\n",
    "\n",
    "# # Resume training from the checkpoint\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tbparse import SummaryReader\n",
    "log_dir = os.path.expanduser('~/asr_project/asr-train/logs')\n",
    "reader = SummaryReader(log_dir)\n",
    "df = reader.scalars\n",
    "df.drop_duplicates(subset=['step','tag'], keep='last', inplace=True)\n",
    "df = df.pivot(index='step', columns='tag', values='value').reset_index()\n",
    "df_eval = df.loc[~pd.isna(df['eval/loss']),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train_epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_epoch'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss vs epochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m----> 9\u001b[0m axs[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(\u001b[43mdf_eval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,df_eval[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval/wer\u001b[39m\u001b[38;5;124m'\u001b[39m],label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_WER\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m axs[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39maxhline(y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.105\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-large-960h_WER\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m axs[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlegend()\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/asr/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_epoch'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_step_per_epoch = 13\n",
    "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "axs[0].plot(df['train/epoch'],df['train/loss'],label='train_loss')\n",
    "axs[0].plot(df_eval['train/epoch'],df_eval['eval/loss'],label='val_loss')\n",
    "axs[0].set_title('Loss vs epochs')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(df_eval['train_epoch'],df_eval['eval/wer'],label='val_WER')\n",
    "axs[1].axhline(y=0.105, color='red', linestyle='--', linewidth=2, label='facebook/wav2vec2-large-960h_WER')\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>tag</th>\n",
       "      <th>step</th>\n",
       "      <th>eval/loss</th>\n",
       "      <th>eval/runtime</th>\n",
       "      <th>eval/samples_per_second</th>\n",
       "      <th>eval/steps_per_second</th>\n",
       "      <th>eval/wer</th>\n",
       "      <th>train/epoch</th>\n",
       "      <th>train/grad_norm</th>\n",
       "      <th>train/learning_rate</th>\n",
       "      <th>train/loss</th>\n",
       "      <th>train/total_flos</th>\n",
       "      <th>train/train_loss</th>\n",
       "      <th>train/train_runtime</th>\n",
       "      <th>train/train_samples_per_second</th>\n",
       "      <th>train/train_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.116986</td>\n",
       "      <td>4.505060</td>\n",
       "      <td>1.245000e-05</td>\n",
       "      <td>5.7395</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>0.348863</td>\n",
       "      <td>543.120422</td>\n",
       "      <td>107.920998</td>\n",
       "      <td>6.746</td>\n",
       "      <td>0.160453</td>\n",
       "      <td>0.233973</td>\n",
       "      <td>4.511003</td>\n",
       "      <td>2.495000e-05</td>\n",
       "      <td>3.0132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.350959</td>\n",
       "      <td>10.393481</td>\n",
       "      <td>3.745000e-05</td>\n",
       "      <td>2.7697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.290724</td>\n",
       "      <td>388.408112</td>\n",
       "      <td>150.908005</td>\n",
       "      <td>9.433</td>\n",
       "      <td>0.142375</td>\n",
       "      <td>0.467946</td>\n",
       "      <td>3.137352</td>\n",
       "      <td>4.985000e-05</td>\n",
       "      <td>3.1198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.584932</td>\n",
       "      <td>3.554898</td>\n",
       "      <td>3.913808e-05</td>\n",
       "      <td>2.5175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>20500</td>\n",
       "      <td>0.204979</td>\n",
       "      <td>546.450989</td>\n",
       "      <td>107.263000</td>\n",
       "      <td>6.705</td>\n",
       "      <td>0.104026</td>\n",
       "      <td>9.592887</td>\n",
       "      <td>3.031529</td>\n",
       "      <td>2.152676e-06</td>\n",
       "      <td>1.9335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>20750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.709873</td>\n",
       "      <td>1.951654</td>\n",
       "      <td>1.539028e-06</td>\n",
       "      <td>1.9213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>21000</td>\n",
       "      <td>0.206288</td>\n",
       "      <td>388.615997</td>\n",
       "      <td>150.828003</td>\n",
       "      <td>9.428</td>\n",
       "      <td>0.103809</td>\n",
       "      <td>9.826860</td>\n",
       "      <td>3.339163</td>\n",
       "      <td>9.253804e-07</td>\n",
       "      <td>1.9053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>21250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.943847</td>\n",
       "      <td>2.509146</td>\n",
       "      <td>3.117329e-07</td>\n",
       "      <td>1.9410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>21370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.424055e+20</td>\n",
       "      <td>0.123629</td>\n",
       "      <td>2610.420898</td>\n",
       "      <td>523.916016</td>\n",
       "      <td>8.186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tag   step  eval/loss  eval/runtime  eval/samples_per_second  \\\n",
       "0      250        NaN           NaN                      NaN   \n",
       "1      500   0.348863    543.120422               107.920998   \n",
       "2      750        NaN           NaN                      NaN   \n",
       "3     1000   0.290724    388.408112               150.908005   \n",
       "4     1250        NaN           NaN                      NaN   \n",
       "..     ...        ...           ...                      ...   \n",
       "87   20500   0.204979    546.450989               107.263000   \n",
       "88   20750        NaN           NaN                      NaN   \n",
       "89   21000   0.206288    388.615997               150.828003   \n",
       "90   21250        NaN           NaN                      NaN   \n",
       "91   21370        NaN           NaN                      NaN   \n",
       "\n",
       "tag  eval/steps_per_second  eval/wer  train/epoch  train/grad_norm  \\\n",
       "0                      NaN       NaN     0.116986         4.505060   \n",
       "1                    6.746  0.160453     0.233973         4.511003   \n",
       "2                      NaN       NaN     0.350959        10.393481   \n",
       "3                    9.433  0.142375     0.467946         3.137352   \n",
       "4                      NaN       NaN     0.584932         3.554898   \n",
       "..                     ...       ...          ...              ...   \n",
       "87                   6.705  0.104026     9.592887         3.031529   \n",
       "88                     NaN       NaN     9.709873         1.951654   \n",
       "89                   9.428  0.103809     9.826860         3.339163   \n",
       "90                     NaN       NaN     9.943847         2.509146   \n",
       "91                     NaN       NaN    10.000000              NaN   \n",
       "\n",
       "tag  train/learning_rate  train/loss  train/total_flos  train/train_loss  \\\n",
       "0           1.245000e-05      5.7395               NaN               NaN   \n",
       "1           2.495000e-05      3.0132               NaN               NaN   \n",
       "2           3.745000e-05      2.7697               NaN               NaN   \n",
       "3           4.985000e-05      3.1198               NaN               NaN   \n",
       "4           3.913808e-05      2.5175               NaN               NaN   \n",
       "..                   ...         ...               ...               ...   \n",
       "87          2.152676e-06      1.9335               NaN               NaN   \n",
       "88          1.539028e-06      1.9213               NaN               NaN   \n",
       "89          9.253804e-07      1.9053               NaN               NaN   \n",
       "90          3.117329e-07      1.9410               NaN               NaN   \n",
       "91                   NaN         NaN      3.424055e+20          0.123629   \n",
       "\n",
       "tag  train/train_runtime  train/train_samples_per_second  \\\n",
       "0                    NaN                             NaN   \n",
       "1                    NaN                             NaN   \n",
       "2                    NaN                             NaN   \n",
       "3                    NaN                             NaN   \n",
       "4                    NaN                             NaN   \n",
       "..                   ...                             ...   \n",
       "87                   NaN                             NaN   \n",
       "88                   NaN                             NaN   \n",
       "89                   NaN                             NaN   \n",
       "90                   NaN                             NaN   \n",
       "91           2610.420898                      523.916016   \n",
       "\n",
       "tag  train/train_steps_per_second  \n",
       "0                             NaN  \n",
       "1                             NaN  \n",
       "2                             NaN  \n",
       "3                             NaN  \n",
       "4                             NaN  \n",
       "..                            ...  \n",
       "87                            NaN  \n",
       "88                            NaN  \n",
       "89                            NaN  \n",
       "90                            NaN  \n",
       "91                          8.186  \n",
       "\n",
       "[92 rows x 15 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfc/anaconda3/envs/asr/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>LEARNED TO RECOGNIZE TH EROMANS AND FOLLOW THEMTHE OLD KING HAD SAID</s>']\n"
     ]
    }
   ],
   "source": [
    "# Load audio file\n",
    "audio_file = \"/home/tfc/asr_project/common_voice/cv-valid-train/cv-valid-train/sample-000000.wav\"  # Replace with your audio file path\n",
    "waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "\n",
    "# If the sample rate is not 16kHz, resample it\n",
    "if sample_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "# Convert to the right format for the model\n",
    "input_values = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "\n",
    "# Get logits from the model\n",
    "model.to('cpu')\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# Get predicted ids\n",
    "predicted_ids = logits.argmax(dim=-1)\n",
    "\n",
    "# Decode the predicted ids to text\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "print(transcription)  # Print the transcription result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path where the fine-tuned model is saved\n",
    "model_dir = os.path.expanduser('~/asr_project/asr-train/model_outputs/checkpoint-2137')\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_dir\n",
    ")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "    return batch\n",
    "\n",
    "model.to('cuda')\n",
    "results = dataset[\"val\"].map(map_to_result, remove_columns=dataset[\"val\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 58614/58614 [00:00<00:00, 73247.36 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 0.121\n"
     ]
    }
   ],
   "source": [
    "def remove_start_end_tags(batch):\n",
    "    # Remove the <s> and </s> tags from both ends of each string in 'pred_str' and 'text'\n",
    "    batch[\"pred_str\"] = re.sub(r\"^<s>|</s>$\", \"\", batch[\"pred_str\"])\n",
    "    batch[\"text\"] = re.sub(r\"^<s>|</s>$\", \"\", batch[\"text\"])\n",
    "    return batch\n",
    "\n",
    "# Apply the function to the entire dataset\n",
    "results = results.map(remove_start_end_tags)\n",
    "\n",
    "\n",
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))\n",
    "# WER of 0.12 for eval set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 136764/136764 [55:19<00:00, 41.20 examples/s] \n"
     ]
    }
   ],
   "source": [
    "def map_to_result(batch):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "    return batch\n",
    "\n",
    "model.to('cuda')\n",
    "results = dataset[\"train\"].map(map_to_result, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 136764/136764 [00:02<00:00, 66451.57 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train WER: 0.120\n"
     ]
    }
   ],
   "source": [
    "def remove_start_end_tags(batch):\n",
    "    # Remove the <s> and </s> tags from both ends of each string in 'pred_str' and 'text'\n",
    "    batch[\"pred_str\"] = re.sub(r\"^<s>|</s>$\", \"\", batch[\"pred_str\"])\n",
    "    batch[\"text\"] = re.sub(r\"^<s>|</s>$\", \"\", batch[\"text\"])\n",
    "    return batch\n",
    "\n",
    "# Apply the function to the entire dataset\n",
    "results = results.map(remove_start_end_tags)\n",
    "\n",
    "\n",
    "print(\"train WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))\n",
    "# WER of 0.12 for train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
